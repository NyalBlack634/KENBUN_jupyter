{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab as mc\n",
    "import mojimoji\n",
    "m = mc.Tagger(\"-Ochasen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    テキストを分かち書きするメソッド\n",
    "    :param text: 分割したいテキスト\n",
    "    :return: 分割後のテキスト\n",
    "    \"\"\"\n",
    "\n",
    "    node = m.parseToNode(text)\n",
    "    word_list = list()\n",
    "    while node:\n",
    "        if node.surface != \"\":\n",
    "            res = node.feature.split(\",\")\n",
    "            word_type = res[0]\n",
    "            if word_type in ['名詞', \"動詞\", \"形容詞\", \"副詞\"]:  # 名詞, 動詞, 形容詞, 副詞のみを抽出\n",
    "                basic_word = res[6]\n",
    "                if basic_word != \"*\":\n",
    "                    word_list.append(basic_word)\n",
    "                else:\n",
    "                    word_list.append('[UNK]')  # 未知語の場合は[UNK]トークンに置き換え\n",
    "        node = node.next\n",
    "        if node is None:\n",
    "            break\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    テキストの正規化\n",
    "    :param text: 正規化するテキスト\n",
    "    :return: 正規化後のテキスト\n",
    "    \"\"\"\n",
    "    text = mojimoji.han_to_zen(text, digit=False, ascii=False)  # 半角文字を全角文字に統一(数字, 英語以外)\n",
    "    text = mojimoji.zen_to_han(text, kana=False)  # 全角文字を半角文字に統一(かな以外)\n",
    "    text = text.lower()  # 小文字に統一\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word(text: str):\n",
    "    \"\"\"\n",
    "    元word_analysis()\n",
    "    ベクトル化を行うメソッド\n",
    "    :param text: ベクトル化するテキスト\n",
    "    :return: ベクトル化したテキスト(array型)\n",
    "    \"\"\"\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    V = list()  # 文章のベクトル(200次元)を格納\n",
    "\n",
    "    # 文章の対して, 文章中の単語のベクトルの平均を求める処理を行う\n",
    "    word_list = mecab_tokenizer(text)\n",
    "    v = np.array([0.0] * 200)\n",
    "    word_num = 0\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            v += np.array(model[word])\n",
    "        except KeyError as error:\n",
    "            continue\n",
    "        except ValueError as verror:\n",
    "            continue\n",
    "        word_num += 1\n",
    "\n",
    "    try:\n",
    "        v = v / word_num\n",
    "    except ZeroDivisionError as e:\n",
    "        print(f'ZeroDivisionError: {e}')\n",
    "\n",
    "    return v\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    \"\"\"\n",
    "    cos類似度を計算\n",
    "    :param v1: word2vecによりベクトル化したテキスト1(ndarray型)\n",
    "    :param v2: word2vecによりベクトル化したテキスト2(ndarray型)\n",
    "    :return: 二つのテキストのcos類似度\n",
    "    \"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec_model.pickle', mode='rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/kenbun/PycharmProjects/ProtKenbunTalk/Excel_News/headline_list.pickle', mode='rb') as f:\n",
    "    healdline_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_keyword = '大分'\n",
    "test_keyword = '縣'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_similarity(headline_list, speech_text: str):\n",
    "    \"\"\"\n",
    "    全ての見出しと発言の類似度分析を行い, 類似度で降順にソートして返す\n",
    "    :param headline_list: 見出しをまとめたリスト<br> ひとつの要素に(ファイルパス, 番号, (見出し, ベクトル)) -> の形で格納している\n",
    "    :param speech_text: 発言\n",
    "    :return: 類似度で降順にソートしたリスト [((会社名, 出版年, 出版付き, 番号, 見出し), 類似度)] -> この形になっている\n",
    "    \"\"\"\n",
    "    # speech_text = '物価下がらなかったのね'\n",
    "    speech_v = vectorize_word(speech_text)  # 発言をベクトル化\n",
    "\n",
    "    similarity_dic = {}\n",
    "\n",
    "    for company, year, month, day, headline, vector in headline_list:\n",
    "        cs = cos_sim(speech_v, vector)  # 発言と見出しテキストのcos類似度を計算\n",
    "\n",
    "        if cs >= 0.75:  # cos類似度が閾値以上であれば提示する見出しリストに加える\n",
    "            similarity_dic[(company, year, month, day, headline)] = cs\n",
    "\n",
    "    return sorted(similarity_dic.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenbun\\anaconda3\\envs\\nlp_bot\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "result = get_all_similarity(healdline_list, test_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "022ca7329b3f1eaea86de9a224d6f432c33c87cffa51bb67d8a9b463ab5dcf0a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('nlp_bot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
